<!DOCTYPE html>
<html>
  <head>
    <title>ETC3250: Classification</title>
    <meta charset="utf-8">
    <meta name="author" content="  Professor Di Cook     Econometrics and Business Statistics   Monash University" />
    <link href="libs/remark-css/kunoichi.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
    <link rel="stylesheet" href="mystyle.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ETC3250: Classification
## Semester 1, 2019
### <br> Professor Di Cook <br> <br> Econometrics and Business Statistics <br> Monash University
### Week 3 (a)

---




class: split-30
layout: false

.column[.pad10px[
## Outline

- .green[Classification]
    - Categorical response

]]
.column[.top50px[

In classification, the output `\(Y\)` is a .orange[categorical variable]. For example,

- Loan approval: `\(Y \in \{successful, unsuccessful\}\)` 
- Type of business culture: `\(Y \in \{clan, adhocracy, market, hierarchical\}\)`
- Historical document author: `\(Y \in \{Austen, Dickens, Imitator\}\)`
- Email: `\(Y \in \{spam, ham\}\)`

Map the categories to a numeric variable, or possibly a binary matrix.

]]

---
layout: true
class: shuriken-full white 

.blade1.bg-green[.content.center.vmiddle[
A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?
]]
.blade2.bg-purple[.content.center.vmiddle[
An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.
]]
.blade3.bg-deep-orange[.content.center.vmiddle[
On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.
]]
.blade4.bg-pink[.content.center.vmiddle[
An email comes into the server. Should it be moved into the inbox or the junk mail box, based on header text, sender, origin, time of day, ...?
]]

---

class: hide-blade2 hide-blade3 hide-blade4 hide-hole

---

class: hide-blade3 hide-blade4 hide-hole
count: false

---

class: hide-blade4 hide-hole
count: false

---

class: hide-hole
count: false

---

count: false 

---
layout: false

class: split-30
layout: false

.column[.pad10px[
## Outline

- .green[Classification]
    - Categorical response
    - Why not linear reg?

]]
.column[.top50px[

&lt;img src="classification_files/figure-html/unnamed-chunk-1-1.png" width="800" style="display: block; margin: auto;" /&gt;

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
]]
.column[.top50px[
&lt;img src="classification_files/figure-html/unnamed-chunk-2-1.png" width="800" style="display: block; margin: auto;" /&gt;
]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
]]
.column[.top50px[

`$$f(x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$`

&lt;img src="classification_files/figure-html/unnamed-chunk-3-1.png" width="600" style="display: block; margin: auto;" /&gt;

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
]]
.column[.top50px[

.split-50[.column[

Transform the function: 

`$$~~~~f(x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$`

`\(\rightarrow  f(x) = \frac{1}{1/e^{\beta_0+\beta_1x}+1}\)`

`\(\rightarrow  1/f(x) = 1/e^{\beta_0+\beta_1x}+1\)`

`\(\rightarrow 1/f(x) - 1 = 1/e^{\beta_0+\beta_1x}\)`

`\(\rightarrow  \frac{1}{1/f(x) - 1} = e^{\beta_0+\beta_1x}\)`

`\(\rightarrow ~~ ...\)`
]
.column[

`\(\rightarrow ~~ ...\)`

`\(\rightarrow \frac{f(x)}{1 - f(x)} = e^{\beta_0+\beta_1x}\)`

`\(\rightarrow \log_e\frac{f(x)}{1 - f(x)} = \beta_0+\beta_1x\)`

&lt;br&gt;
&lt;br&gt;


The left-hand side `\(\log_e\frac{f(x)}{1 - f(x)}\)` is called the .orange[log-odds ratio] or logit.

<span>&lt;i class="fas  fa-dice fa-lg faa-bounce animated faa-slow " style=" color:orange;"&gt;&lt;/i&gt;</span>

]]]

]]
---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
]]
.column[.top50px[

.boxshadow[
The fitted model is then written as:

.content[
`\(\log_e\frac{P(Y=1|X)}{1 - P(Y=1|X)} = \beta_0+\beta_1X\)`
]]

and then 

`$$P(Y=0|X) = 1 - P(Y=1|X)$$`

*Multiple categories*: This formula can be extended to more than binary response variables. Writing the equation is not simple, but follows from the above, extending it to provide probabilities for each level/category. The sum of all  probabilities is 1.



]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
    - Interpretation
]]
.column[.top50px[

- Linear regression	
    - `\(\beta_1\)` gives the average change in `\(Y\)` associated with a one-unit increase in `\(X\)`
- Logistic regression
    - Increasing `\(X\)` by one unit changes the log odds by `\(\beta_1\)`, or equivalently it multiplies the odds by `\(e^{\beta_1}\)`
    - However, because the model is not linear in `\(X\)`, `\(\beta_1\)` does not correspond to the change in response associated with a one-unit increase in `\(X\)`
    
]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
    - Interpretation
    - Estimation
]]
.column[.top50px[

Maximum Likelihood Estimation

Given the logistic `\(p(x_i) = \frac{1}{e^{-(\beta_0+\beta_1x})+1}\)`

We choose parameters `\(\beta_0, \beta_1\)` to maximize the likelihood of the data given the model. The likelihood function is

`$$\mathcal{l}_n(\beta_0, \beta_1) = \prod_{y_i=1,i}^n p(x_i)\prod_{y_i=0,i}^n (1-p(x_i)).$$`

It is more convenient to maximize the *log-likelihood*:

`$$\text{max}_{\beta_0, \beta_1} ~ l_n(\beta_0, \beta_1) =  - \sum_{i = 1}^n \log(1 + e^{-(\beta_0+\beta_1x}) )$$`


]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
    - Interpretation
    - Estimation
    - Making predictions
]]
.column[.top50px[

With estimates from the model fit, `\(\hat{\beta_0}, \hat{\beta_1}\)`, predict the response using:


`$$\hat{p}(x) = \frac{e^{\hat{\beta_0}+ \hat{\beta_1}x}}{1+e^{\hat{\beta_0}+ \hat{\beta_1}x}}$$`

It is a proportion, but can be rounded to 0 or 1 for class prediction.

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
    - Interpretation
    - Estimation
    - Making predictions
    - Example
]]
.column[.top50px[

Simulated data to predict which customers will default on their credit card debt.

&lt;img src="classification_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /&gt;

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
    - Interpretation
    - Estimation
    - Making predictions
    - Example
]]
.column[.top50px[


```r
library(broom)
fit &lt;- glm(default~balance, data=simcredit, family="binomial")
tidy(fit)
```

```
## # A tibble: 2 x 5
##   term         estimate std.error statistic   p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) -10.7      0.361        -29.5 3.62e-191
## 2 balance       0.00550  0.000220      25.0 1.98e-137
```

<span>&lt;i class="fas  fa-broom fa-lg faa-passing animated " style=" color:orange;"&gt;&lt;/i&gt;</span>

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
    - Interpretation
    - Estimation
    - Making predictions
    - Example
]]
.column[.top50px[


```r
glance(fit)
```

```
## # A tibble: 1 x 7
##   null.deviance df.null logLik   AIC   BIC deviance df.residual
##           &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;
## 1         2921.    9999  -798. 1600. 1615.    1596.        9998
```
]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- .green[Logistic regression]
    - Logistic function
    - Logistic model - GLM
    - Interpretation
    - Estimation
    - Making predictions
    - Example
]]
.column[.top50px[


```r
simcredit_fit &lt;- augment(fit, simcredit, type.predict="response")
ggplot(simcredit_fit, aes(x=balance, y=default_bin)) + 
  geom_point() +
  geom_ribbon(aes(ymin=.fitted-2*.se.fit, ymax=.fitted+2*.se.fit), 
              fill = "orange", alpha=0.3) +
  geom_line(aes(y=.fitted), colour = "orange") + 
  ylab("default")
```

&lt;img src="classification_files/figure-html/unnamed-chunk-9-1.png" width="500" style="display: block; margin: auto;" /&gt;
]]
---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
]]
.column[.top50px[

Logistic regression involves directly modeling `\(Pr(Y = k|X = x)\)` using the logistic function. Rounding the probabilities produces class predictions, in two class problems; selecting the class with the highest probability produces class predictions in multi-class problems.

Another approach for building a classification model is .orange[linear discriminant analysis]. This involves directly estimating the .orange[distribution of the predictors], separately for each class.

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem

]]
.column[.top50px[

Let `\(f_k(x)\)` be the density function for predictor `\(x\)` for class `\(k\)`. If `\(f\)` is small, the probability that `\(x\)` belongs to class `\(k\)` is small, and conversely if `\(f\)` is large.

Bayes theorem (for `\(K\)` classes) states:

.boxshadow[
.content[

`$$Pr(Y = k|X = x) = p_k(x) = \frac{\pi_kf_k(x)}{\sum_{i=1}^K \pi_kf_k(x)}$$`
]]

where `$$\pi_k = \text{Pr}(Y = k)$$` is the prior probability that the observation comes from class, *k*. 

]]
---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
]]
.column[.top50px[

&lt;img src="classification_files/figure-html/unnamed-chunk-10-1.png" width="800" style="display: block; margin: auto;" /&gt;

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
]]
.column[.top50px[

We assume `\(f_k(x)\)` is .orange[Normal] or Gaussian:

`$$f_k(x) = \frac{1}{\sqrt{2 \pi} \sigma_k} \text{exp}~ \left( - \frac{1}{2 \sigma^2_k} (x - \mu_k)^2 \right)$$`

where `\(\mu_k\)` and `\(\sigma^2_k\)` are the mean and variance parameters for the `\(k\)`th class. Further assume that `\(\sigma_1^2 = \sigma_2^2 = \dots = \sigma_K^2\)`; then the conditional probabilities are 

`$$p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} \text{exp}~ \left( - \frac{1}{2 \sigma^2} (x - \mu_k)^2 \right) }{ \sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} \text{exp}~ \left( - \frac{1}{2 \sigma^2} (x - \mu_l)^2 \right) }$$`


]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
]]
.column[.top50px[
The Bayes classifier is assign new observation `\(X=x_0\)` to the class with the highest `\(p_k(x_0)\)`. A simplification of `\(p_k(x_0)\)` yields the .orange[discriminant functions]: 

`$$\delta_k(x_0) = x_0 \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2} + log(\pi_k)$$`
and the rule Bayes classifier will assign `\(x_0\)` to the class with the largest value. 

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
]]
.column[.top50px[

When `\(K = 2\)` and `\(\pi_1 = \pi_2\)`, then the Bayes classifier is :

*Assign* `\(x_0\)` *to class 1, if* 

`$$\delta_1(x_0) &gt; \delta_2(x_0)$$`

$$x_0 \frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2 \sigma^2} + log(\pi) &gt; x_0 \frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2 \sigma^2} + log(\pi) $$

which simplifies to 

`$$x_0 &gt; \frac{\mu_1+\mu_2}{2}$$`
.boxshadow[
.content[
This is estimated on the data with 
`\(x_0 &gt; \frac{\bar{x}_1 + \bar{x}_2}{2}\)`.
]]

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
    - Multivariate
]]
.column[.top50px[

To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution with `\(E[X] = \mu\)` and `\(\text{Cov}(X) = \Sigma\)`, we write `\(X \sim N(\mu, \Sigma)\)`.

The multivariate normal density function is:

`$$f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\{-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu)\}$$`

with `\(x, \mu\)` are `\(p\)`-dimensional vectors, `\(\Sigma\)` is a `\(p\times p\)` variance-covariance matrix. 

]]
---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
    - Multivariate
]]
.column[.top50px[
The discriminant functions are:

`$$\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k'\Sigma^{-1}\mu_k + \pi_k$$`

and Bayes classifier is .orange[assign a new observation] `\(x_0\)` .orange[to the class with the highest] `\(\delta_k(x_0)\)`.

When `\(K=2\)` and `\(\pi_1=\pi_2\)` this reduces to 

Assign observation `\(x_0\)` to class 1 if 

`$$x_0'\Sigma^{-1}(\mu_1-\mu_2) &gt; \frac{1}{2}(\mu_1+\mu_2)'\Sigma^{-1}(\mu_1-\mu_2)$$`

]]
---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
    - Multivariate
]]
.column[.top50px[

.orange[Discriminant space]: a benefit of LDA is that it provides a low-dimensional projection of the `\(p\)`-dimensional space, where the groups are the most separated. For `\(K=2\)`, this is

`$$\Sigma^{-1}(\mu_1-\mu_2)$$`

For `\(K&gt;2\)`, the discriminant space is found be taking an eigen-decomposition of `\(\Sigma^{-1}\Sigma_B\)`, where

`$$\Sigma_B = \frac{1}{K}\sum_{i=1}^{K} (\mu_i-\mu)(\mu_i-\mu)'$$`

]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
    - Multivariate
]]
.column[.top50px[



The dashed lines are the Bayes decision boundaries. Ellipses
that contain 95% of the probability for each of the three classes are shown. Solid line corresponds to the class boundaries from the LDA model fit to the sample.

&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.6.pdf" target="_BLANK"&gt; &lt;img src="images/4.6.png" style="width: 90%; align: center"/&gt; &lt;/a&gt;

.font_tiny[(Chapter4/4.6.pdf)]
]]

---
class: split-30

.column[.pad10px[
## Outline

- Classification
- Logistic regression
- .green[Linear discriminant analysis]
    - Bayes theorem
    - When `\(p=1\)`
    - Multivariate
    - Quadratic
]]
.column[.top50px[

A quadratic boundary is obtained by relaxing the assumption of equal variance-covariance, and assume that `\(\Sigma_k \neq \Sigma_l, ~~k\neq l, k,l=1,...,K\)`



&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.9.pdf" target="_BLANK"&gt; &lt;img src="images/4.9.png" style="width: 90%; align: center"/&gt; &lt;/a&gt;

.font_tiny[(Chapter4/4.9.pdf)]

]]
---
layout: false
# 👩‍💻 Made by a human with a computer

### Slides at [https://monba.dicook.org](https://monba.dicook.org).
### Code and data at [https://github.com/dicook/Business_Analytics](https://github.com/dicook/Business_Analytics).
&lt;br&gt;

### Created using [R Markdown](https://rmarkdown.rstudio.com) with flair by [**xaringan**](https://github.com/yihui/xaringan), and [**kunoichi** (female ninja) style](https://github.com/emitanaka/ninja-theme).

&lt;br&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
